import os
import csv
import time
import subprocess
import logging
import statistics
import gc
import psutil
import sys
from typing import List, Tuple, Optional

# T·∫Øt log
logging.getLogger().setLevel(logging.CRITICAL)

# Output files
OUTPUT_CSV = "benchmark_runtime_only.csv"
OUTPUT_DETAILED = "benchmark_runtime_detailed.csv"

# C·∫•u h√¨nh benchmark
class BenchmarkConfig:
    WARMUP_RUNS = 3              # S·ªë l·∫ßn ch·∫°y warm-up
    MEASUREMENT_RUNS = 10        # S·ªë l·∫ßn ƒëo ch√≠nh th·ª©c
    TIMEOUT_SECONDS = 5          # Timeout cho m·ªói l·∫ßn ch·∫°y
    OUTLIER_THRESHOLD = 2.5      # Z-score ƒë·ªÉ lo·∫°i outliers
    CPU_AFFINITY = 0             # CPU core c·ªë ƒë·ªãnh (None = kh√¥ng set)
    HIGH_PRIORITY = True         # Ch·∫°y v·ªõi priority cao
    IDLE_TIME = 0.1              # Th·ªùi gian ch·ªù gi·ªØa c√°c l·∫ßn ƒëo (gi√¢y)
    MIN_VALID_RUNS = 5           # S·ªë l·∫ßn ch·∫°y t·ªëi thi·ªÉu ƒë·ªÉ k·∫øt qu·∫£ h·ª£p l·ªá

def set_process_priority():
    """ƒê·∫∑t priority cao cho process benchmark"""
    if not BenchmarkConfig.HIGH_PRIORITY:
        return
    
    try:
        p = psutil.Process(os.getpid())
        if sys.platform == "win32":
            p.nice(psutil.HIGH_PRIORITY_CLASS)
        else:
            p.nice(-10)  # Unix/Linux (c·∫ßn sudo)
    except Exception as e:
        print(f"[!] Kh√¥ng th·ªÉ set priority cao: {e}")

def set_cpu_affinity():
    """G√°n process v√†o CPU core c·ªë ƒë·ªãnh"""
    if BenchmarkConfig.CPU_AFFINITY is None:
        return
    
    try:
        p = psutil.Process(os.getpid())
        p.cpu_affinity([BenchmarkConfig.CPU_AFFINITY])
    except Exception as e:
        print(f"[!] Kh√¥ng th·ªÉ set CPU affinity: {e}")

def wait_for_system_idle():
    """ƒê·ª£i h·ªá th·ªëng ·ªïn ƒë·ªãnh tr∆∞·ªõc khi ƒëo"""
    time.sleep(BenchmarkConfig.IDLE_TIME)
    gc.collect()
    gc.disable()  # T·∫Øt GC trong khi ƒëo

def restore_system_state():
    """Kh√¥i ph·ª•c tr·∫°ng th√°i h·ªá th·ªëng sau khi ƒëo"""
    gc.enable()
    gc.collect()

def remove_outliers(values: List[float]) -> List[float]:
    """Lo·∫°i b·ªè outliers d√πng Z-score"""
    if len(values) < 3:
        return values
    
    mean = statistics.mean(values)
    stdev = statistics.stdev(values)
    
    if stdev == 0:
        return values
    
    z_scores = [(x - mean) / stdev for x in values]
    filtered = [v for v, z in zip(values, z_scores) 
                if abs(z) < BenchmarkConfig.OUTLIER_THRESHOLD]
    
    # ƒê·∫£m b·∫£o c√≤n ƒë·ªß s·ªë l·∫ßn ƒëo
    if len(filtered) < BenchmarkConfig.MIN_VALID_RUNS:
        # N·∫øu lo·∫°i qu√° nhi·ªÅu, gi·ªØ l·∫°i MIN_VALID_RUNS gi√° tr·ªã g·∫ßn mean nh·∫•t
        sorted_by_distance = sorted(values, key=lambda x: abs(x - mean))
        return sorted_by_distance[:BenchmarkConfig.MIN_VALID_RUNS]
    
    return filtered

def single_runtime_measurement(path: str) -> Tuple[float, bool]:
    """
    ƒêo runtime m·ªôt l·∫ßn
    Returns: (runtime_ms, is_valid)
    """
    wait_for_system_idle()
    
    try:
        # D√πng perf_counter_ns cho ƒë·ªô ch√≠nh x√°c cao nh·∫•t
        start_ns = time.perf_counter_ns()
        
        # C·∫•u h√¨nh subprocess ƒë·ªÉ gi·∫£m overhead
        startupinfo = None
        if os.name == 'nt':
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
            startupinfo.wShowWindow = subprocess.SW_HIDE
        
        result = subprocess.run(
            [path],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            stdin=subprocess.DEVNULL,
            timeout=BenchmarkConfig.TIMEOUT_SECONDS,
            startupinfo=startupinfo,
            creationflags=(subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0)
        )
        
        end_ns = time.perf_counter_ns()
        runtime_ms = (end_ns - start_ns) / 1_000_000.0
        
        restore_system_state()
        
        # Ki·ªÉm tra exit code
        if result.returncode != 0:
            print(f"  [!] Exit code kh√°c 0: {result.returncode}")
            return runtime_ms, False
        
        return runtime_ms, True
        
    except subprocess.TimeoutExpired:
        restore_system_state()
        return -1, False
    except Exception as e:
        restore_system_state()
        print(f"  [!] L·ªói: {e}")
        return -2, False

def benchmark_runtime_with_stats(path: str) -> Optional[dict]:
    """
    Ch·∫°y benchmark nhi·ªÅu l·∫ßn v√† t√≠nh statistics
    Returns: dict v·ªõi c√°c metrics ho·∫∑c None n·∫øu l·ªói
    """
    print(f"\n[*] Benchmarking: {os.path.basename(path)}")
    
    # Warm-up runs
    print(f"  Warm-up ({BenchmarkConfig.WARMUP_RUNS} runs)...", end='', flush=True)
    for i in range(BenchmarkConfig.WARMUP_RUNS):
        runtime, _ = single_runtime_measurement(path)
        print("." if runtime > 0 else "x", end='', flush=True)
    print(" done")
    
    # Measurement runs
    valid_runs = []
    failed_runs = 0
    
    print(f"  Measuring ({BenchmarkConfig.MEASUREMENT_RUNS} runs)...", end='', flush=True)
    for i in range(BenchmarkConfig.MEASUREMENT_RUNS):
        runtime, is_valid = single_runtime_measurement(path)
        
        if runtime > 0 and is_valid:
            valid_runs.append(runtime)
            print(".", end='', flush=True)
        else:
            failed_runs += 1
            print("x", end='', flush=True)
    print(" done")
    
    if len(valid_runs) < BenchmarkConfig.MIN_VALID_RUNS:
        print(f"  [!] Kh√¥ng ƒë·ªß l·∫ßn ch·∫°y h·ª£p l·ªá ({len(valid_runs)}/{BenchmarkConfig.MIN_VALID_RUNS})")
        return None
    
    # Lo·∫°i outliers
    filtered_runs = remove_outliers(valid_runs)
    outliers_removed = len(valid_runs) - len(filtered_runs)
    
    # T√≠nh statistics
    stats = {
        'mean': statistics.mean(filtered_runs),
        'median': statistics.median(filtered_runs),
        'stdev': statistics.stdev(filtered_runs) if len(filtered_runs) > 1 else 0,
        'min': min(filtered_runs),
        'max': max(filtered_runs),
        'valid_runs': len(filtered_runs),
        'total_runs': BenchmarkConfig.MEASUREMENT_RUNS,
        'outliers_removed': outliers_removed,
        'cv': 0  # Coefficient of variation
    }
    
    # T√≠nh CV (ƒë·ªô bi·∫øn thi√™n t∆∞∆°ng ƒë·ªëi)
    if stats['mean'] > 0:
        stats['cv'] = (stats['stdev'] / stats['mean']) * 100
    
    print(f"  Mean: {stats['mean']:.3f}ms ¬± {stats['stdev']:.3f}ms (CV: {stats['cv']:.1f}%)")
    print(f"  Valid runs: {stats['valid_runs']}/{stats['total_runs']}, Outliers: {stats['outliers_removed']}")
    
    return stats

def percent_diff(new, old):
    return 0.0 if old == 0 else (new - old) / old * 100.0

def find_original_binaries(root="."):
    for subdir, _, files in os.walk(root):
        for f in files:
            if f.endswith(".exe") and ".cff." not in f and ".junk." not in f:
                yield os.path.join(subdir, f)

def write_csv_headers():
    """T·∫°o headers cho c·∫£ 2 file CSV"""
    # CSV ch√≠nh (summary)
    fieldnames_main = [
        "Original", "Variant", "Type",
        "runtime_orig_mean", "runtime_variant_mean", "runtime_diff(%)",
        "orig_cv(%)", "variant_cv(%)"
    ]
    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as f:
        csv.DictWriter(f, fieldnames=fieldnames_main).writeheader()
    
    # CSV chi ti·∫øt
    fieldnames_detail = [
        "Binary", "Type", "Mean", "Median", "StdDev", "Min", "Max",
        "CV(%)", "ValidRuns", "TotalRuns", "OutliersRemoved"
    ]
    with open(OUTPUT_DETAILED, "w", newline="", encoding="utf-8") as f:
        csv.DictWriter(f, fieldnames=fieldnames_detail).writeheader()

def append_detailed_stats(binary_path, binary_type, stats):
    """Ghi statistics chi ti·∫øt"""
    row = {
        "Binary": os.path.basename(binary_path),
        "Type": binary_type,
        "Mean": round(stats['mean'], 3),
        "Median": round(stats['median'], 3),
        "StdDev": round(stats['stdev'], 3),
        "Min": round(stats['min'], 3),
        "Max": round(stats['max'], 3),
        "CV(%)": round(stats['cv'], 2),
        "ValidRuns": stats['valid_runs'],
        "TotalRuns": stats['total_runs'],
        "OutliersRemoved": stats['outliers_removed']
    }
    with open(OUTPUT_DETAILED, "a", newline="", encoding="utf-8") as f:
        csv.DictWriter(f, fieldnames=row.keys()).writerow(row)

def append_comparison_row(orig_path, variant_path, variant_type, orig_stats, variant_stats):
    """Ghi so s√°nh v√†o CSV ch√≠nh"""
    row = {
        "Original": os.path.basename(orig_path),
        "Variant": os.path.basename(variant_path),
        "Type": variant_type,
        "runtime_orig_mean": round(orig_stats['mean'], 3),
        "runtime_variant_mean": round(variant_stats['mean'], 3),
        "runtime_diff(%)": round(percent_diff(variant_stats['mean'], orig_stats['mean']), 2),
        "orig_cv(%)": round(orig_stats['cv'], 2),
        "variant_cv(%)": round(variant_stats['cv'], 2)
    }
    with open(OUTPUT_CSV, "a", newline="", encoding="utf-8") as f:
        csv.DictWriter(f, fieldnames=row.keys()).writerow(row)

def main():
    print(f"=== Enhanced Runtime Benchmark ===")
    print(f"Warm-up runs: {BenchmarkConfig.WARMUP_RUNS}")
    print(f"Measurement runs: {BenchmarkConfig.MEASUREMENT_RUNS}")
    print(f"Outlier threshold: {BenchmarkConfig.OUTLIER_THRESHOLD} œÉ")
    print(f"CPU affinity: {BenchmarkConfig.CPU_AFFINITY}")
    print(f"High priority: {BenchmarkConfig.HIGH_PRIORITY}")
    
    # Setup h·ªá th·ªëng
    set_process_priority()
    set_cpu_affinity()
    
    # T·∫°o CSV headers
    write_csv_headers()
    
    # Benchmark
    for orig in find_original_binaries("."):
        orig_stats = benchmark_runtime_with_stats(orig)
        if not orig_stats:
            print(f"[!] B·ªè qua {orig} do l·ªói benchmark")
            continue
        
        # Ghi stats chi ti·∫øt c·ªßa original
        append_detailed_stats(orig, "original", orig_stats)
        
        base_name, _ = os.path.splitext(orig)
        for suffix, variant_type in [(".cff.exe", "cff"), (".junk.exe", "junk")]:
            variant_path = base_name + suffix
            if not os.path.exists(variant_path):
                continue
            
            variant_stats = benchmark_runtime_with_stats(variant_path)
            if not variant_stats:
                print(f"[!] B·ªè qua {variant_path} do l·ªói benchmark")
                continue
            
            # Ghi stats chi ti·∫øt c·ªßa variant
            append_detailed_stats(variant_path, variant_type, variant_stats)
            
            # Ghi comparison
            append_comparison_row(orig, variant_path, variant_type, orig_stats, variant_stats)
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            diff = percent_diff(variant_stats['mean'], orig_stats['mean'])
            print(f"[‚úì] {variant_type.upper()}: {diff:+.2f}% "
                  f"({orig_stats['mean']:.3f}ms ‚Üí {variant_stats['mean']:.3f}ms)")

    print(f"\n‚úÖ Ho√†n t·∫•t!")
    print(f"üìä K·∫øt qu·∫£ summary: {OUTPUT_CSV}")
    print(f"üìä K·∫øt qu·∫£ chi ti·∫øt: {OUTPUT_DETAILED}")

if __name__ == "__main__":
    main()